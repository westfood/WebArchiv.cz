{\rtf1\ansi\ansicpg1252\uc1\deff0
{\fonttbl{\f0\fnil\fcharset238\fprq2 Courier New;}{\f1\fnil\fcharset238\fprq2 Segoe UI;}}
{\colortbl;\red0\green0\blue0;}
\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\fet2\ftnbj\aenddoc
\pgnrestart\pgnstarts0
\pard\plain \fi0\ltrch\loch {\f1\fs24 In 2011 I get access to two physical machines which were harvesting web since 2008. Each one have been fitted with 20 TB RAID-6 storage and 8 GB RAM.  First crawler was dedicated for regular crawls, second one for whole domain or topic crawling. It actually worked quite well for our purposes, but biggest drawback of such approach was lengthy domain crawling.  Sometimes it took 3 or more months to crawl around 800k seeds and capture 5000 responses per domain. During such long crawl we encountered many unexpected incidents such as power blackout, Internet disconnection or corrupting crawl by running out of RAM . Regular checkpointing was therefore necessary to recover from failed crawl. But it was also necessary to pause crawl for checkpoint and it took unpleasant while before checkpoint has been written to disk and I could resume crawl. And in the end I also experienced that checkpoints failed to recover from time to time. Longer you crawl, easier you run into incident you need to deal with. There was even another argument for reducing time of our domain crawl. Libor Coufal who was head of Web Archive department in these times stressed that more condensed timeframe of harvest results in better validity of captured documents from curator\hich\f1 \rquote \loch\f1 s perspective. Memento presentation I attended two years later proved him right to level that discrepancies between captured documents, could result in archived content which does not represent reality. }
\par\plain {\f1\fs24 Luckily maintenance of our servers has been running out and easiest solution seemed to run Heritrix as virtualized crawler. Library already invested in VMware ESX infrastructure, so whole bunch of old machines, Web Archive\hich\f1 \rquote \loch\f1 s web and all curator\hich\f1 \rquote \loch\f1 s application has been virtualized.  There was still quite a lot of capacity for other projects and our crawlers especially. So I prepared first three crawlers with 2 vCPUs and 8 GB RAM and run testing crawl. I didn\hich\f1 \rquote \loch\f1 t do any benchmarking,  crawls just behaved well and performance felt right. So I ended up with six crawlers and possibility to dedicate five of them for domain crawling. }
\par\plain {\f1\fs24 For domain crawl I obtained Czech domain names from IXP nix.cz and split them as seeds into five parts and fed them to each crawler. Obvious problem was crawlers doesn\hich\f1 \rquote \loch\f1 t have shared URI queue, so they will eventually harvest identical URIs founded during crawl, increasing overall harvest size as result. Anyway it was accepted behavior in sake of faster domain crawl. }
\par\plain {\f1\fs24 But new incident occurred during first week of crawl, crawlers ceased to write ARCs and end up in errors about no response from storage service. Before final collapse of crawl, errors were coming in waves. Heritrix deals quite nice with slow responses from storage, giving up writing for while, trying to write again later.  He care for storage in same way as he care for web servers, it is very gentle code. But in the end, crawlers died.  }
\par\plain {\f1\fs24 After quick research I found cause in old pal NFS.  Old crawlers harvested ARCs to local storage and then I moved data to SAN over NFS, checking hashes after transfer. Once crawlers have been virtualized, I started to harvest straight to SAN over NFS. It seemed like jolly good approach to me, but I didn\hich\f1 \rquote \loch\f1 t knew NFS have issues with too many data connections. After bit of experimenting I concluded that our NFS server configuration can handle only two crawlers without any errors. Quickest and cheapest options was to spread crawlers writing across three NFS servers. Which worked perfectly with our SAN configuration, because third NFS has been just installed and they are only access points into our storage.}
\par\plain \f1\fs24
\par\plain \f1\fs24
\par\plain \f1\fs24
\par\plain {\f1\fs24 Initializing Git repository:}
\par\plain {\f1\fs24\b0\i0 heritrix@crawler00:/opt/heritrix/jobs> }
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git clone git://github.com/WebArchivCZ/Crawler-config.git}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 mkdir Crawler-config/Domain-crawls}
\par\plain {\f1\fs24\b0\i0 cd Crawler-config/Domain-crawls}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git checkout -b \hich\f1 \ldblquote \loch\f1 Metadata\hich\f1 \rdblquote \loch\f1 }
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 vim crawler-beans.cxml}
\par\plain {\f1\fs24\b0\i0 # Values changing with each crawl\hich\f1 \rquote \loch\f1 }
\par\plain {\f1\fs24\b0\i0 metadata.jobName=Serials 2013-01-1M}
\par\plain {\f1\fs24\b0\i0 metadata.description=Pravideln\loch\af1\hich\af1\dbch\af1\uc1\u225\'E1 sklize\u328\'F2 sem\u237\'EDnek s ka\u382\'9Edom\u283\'ECs\u237\'ED\u269\'E8n\u237\'ED frekvenc\u237\'ED}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 # Stable values}
\par\plain {\f1\fs24\b0\i0 metadata.operatorContactUrl=http://webarchiv.cz/kontakty/}
\par\plain {\f1\fs24\b0\i0 metadata.operator=Rudolf Kreibich}
\par\plain {\f1\fs24\b0\i0 metadata.operatorFrom=webarchiv@nkp.cz}
\par\plain {\f1\fs24\b0\i0 metadata.organization=National Library of the Czech Republic - WebArchiv.cz}
\par\plain {\f1\fs24\b0\i0 metadata.audience=WebArchiv.cz Users}
\par\plain {\f1\fs24\b0\i0 metadata.userAgentTemplate=Mozilla/5.0 (compatible; heritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)}
\par\plain {\f1\fs24\b0\i0 metadata.robotsPolicyName=ignore}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git add crawler-beans.cxml}
\par\plain {\f1\fs24\b0\i0 git -a -m "Metadata configuration for crawling"}
\par\plain {\f1\fs24\b0\i0 git commit -a -m "Metadata configuration for crawling"}
\par\plain {\f1\fs24\b0\i0 git checkout master}
\par\plain {\f1\fs24\b0\i0 git merge Metadata}
\par\plain {\f1\fs24\b0\i0 git branch -d Metadata}
\par\plain \f1\fs24\b0\i0
\par\plain \f1\fs24\b0\i0}