{\rtf1\ansi\ansicpg1252\uc1\deff0
{\fonttbl{\f0\fnil\fcharset238\fprq2 Courier New;}{\f1\fnil\fcharset238\fprq2 Segoe UI;}}
{\colortbl;\red0\green0\blue0;}
\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\fet2\ftnbj\aenddoc
\pgnrestart\pgnstarts0
\pard\plain \fi0\ltrch\loch {\f1\fs24 When I get into webarchiving, we harvested web with two physical machines, each fitted with 20 TB RAID-6 storage,.  First crawler dedicated for regular crawls and second for whole domain or topic crawling. It actually worked quite well for our purposes, but biggest drawback of such approach was lengthy domain crawling.  It took sometimes 3+ month to crawl around 800k seeds and capture 5000 responses per domain. During such long time we could encountered many unexpected situation such as power blackout, DNS or Internet connection problems. Also there was small chance running Heritrix into out of RAM situation resulting in corruption of crawl. Without duplication reduction mechanism, checkpointing was necessary but bitterly time consuming in Heritrix 1. And on top of that checkpointed crawl failed to recover from time to time. Another argument for shortening our domain crawl came from curators perspective. Archived content should be in as condensed timeframe as possible, because we care about coherence of our time slice of web in particular.}
\par\plain {\f1\fs24 Easiest solution seemed to run Heritrix as virtualized crawler. Library already invested in VMware ESX infrastructure, so whole bunch of old machines, webarchive\hich\f1 \rquote \loch\f1 s web and all curator\hich\f1 \rquote \loch\f1 s application has been virtualized before my arrival.  There was still quite a lot of capacity for of other projects and our crawlers. So I prepared first three crawlers with 2 vCPUs and 8 GB RAM and crawler performance seemed fine. There were no hard testing with comparation of numbers between virtual and physical machine, crawl just seemed to run as expected.  So I ended up with six crawlers and possibility to dedicate five of them for domain crawling.}
\par\plain {\f1\fs24 So I split seeds into five parts and feed them to each crawler. Obvious problem was crawlers doesn\hich\f1 \rquote \loch\f1 t have shared URI queue, so they will eventually harvest identical URIs founded during crawl and increasing overall harvest size. Anyway it was accepted behavior in sake of faster domain crawl. But new incident occurred during first week of crawl, our crawlers ceased to write ARCs and end up in errors about no response from storage service. Before final collapse of crawl, errors were coming in waves. Heritrix have quite nice way how to deal with slow responses from storage, giving up writing for while, trying to write again later.  It actually looks like same gentle approach it has towards web servers. But in the end, crawlers died anyway.  After quick research I found cause in old pal NFS. }
\par\plain {\f1\fs24 Before virtualization we harvested ARCs to local storage and then we moved data to SAN over NFS. Once crawlers have been virtualized, I started to harvest straight to SAN over NFS. It seemed like jolly good approach to me, but I didn\hich\f1 \rquote \loch\f1 t knew NFS have issues with too many data connections. After bit of experimenting I concluded that our NFS server configuration can handle only two crawlers without any errors. Quickest and cheapest options was to spread crawlers writing across three NFS servers. Which worked perfectly with our SAN configuration, because third NFS has been just installed and they are only access points into our storage.}
\par\plain \f1\fs24
\par\plain \f1\fs24
\par\plain \f1\fs24
\par\plain {\f1\fs24 Initializing Git repository:}
\par\plain {\f1\fs24\b0\i0 heritrix@crawler00:/opt/heritrix/jobs> }
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git clone git://github.com/WebArchivCZ/Crawler-config.git}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 mkdir Crawler-config/Domain-crawls}
\par\plain {\f1\fs24\b0\i0 cd Crawler-config/Domain-crawls}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git checkout -b \hich\f1 \ldblquote \loch\f1 Metadata\hich\f1 \rdblquote \loch\f1 }
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 vim crawler-beans.cxml}
\par\plain {\f1\fs24\b0\i0 # Values changing with each crawl\hich\f1 \rquote \loch\f1 }
\par\plain {\f1\fs24\b0\i0 metadata.jobName=Serials 2013-01-1M}
\par\plain {\f1\fs24\b0\i0 metadata.description=Pravideln\loch\af1\hich\af1\dbch\af1\uc1\u225\'E1 sklize\u328\'F2 sem\u237\'EDnek s ka\u382\'9Edom\u283\'ECs\u237\'ED\u269\'E8n\u237\'ED frekvenc\u237\'ED}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 # Stable values}
\par\plain {\f1\fs24\b0\i0 metadata.operatorContactUrl=http://webarchiv.cz/kontakty/}
\par\plain {\f1\fs24\b0\i0 metadata.operator=Rudolf Kreibich}
\par\plain {\f1\fs24\b0\i0 metadata.operatorFrom=webarchiv@nkp.cz}
\par\plain {\f1\fs24\b0\i0 metadata.organization=National Library of the Czech Republic - WebArchiv.cz}
\par\plain {\f1\fs24\b0\i0 metadata.audience=WebArchiv.cz Users}
\par\plain {\f1\fs24\b0\i0 metadata.userAgentTemplate=Mozilla/5.0 (compatible; heritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)}
\par\plain {\f1\fs24\b0\i0 metadata.robotsPolicyName=ignore}
\par\plain \f1\fs24\b0\i0
\par\plain {\f1\fs24\b0\i0 git add crawler-beans.cxml}
\par\plain {\f1\fs24\b0\i0 git -a -m "Metadata configuration for crawling"}
\par\plain {\f1\fs24\b0\i0 git commit -a -m "Metadata configuration for crawling"}
\par\plain {\f1\fs24\b0\i0 git checkout master}
\par\plain {\f1\fs24\b0\i0 git merge Metadata}
\par\plain {\f1\fs24\b0\i0 git branch -d Metadata}
\par\plain \f1\fs24\b0\i0
\par\plain \f1\fs24\b0\i0}