{\rtf1\ansi\ansicpg1252\deff0
{\fonttbl{\f0\fnil\fprq2 Courier New;}}
{\colortbl;\red0\green0\blue0;\red0\green0\blue128;}
\paperw12240\paperh15840\margl1800\margr1800\margt1440\margb1440\fet2\ftnbj\aenddoc
\pgnrestart\pgnstarts0
\pard\plain \fi720\ltrch\loch {\f0\fs24\b0\i0 Our broad crawls within .cz domain counts around 1M initial seeds. Such crawls had been made in past years usually on one physical machine with attached RAID 6 storage. It was necessary to make checkpoints during crawl as Heritrix used to crash from various reasons starting from lack of RAM to failed components in our infrastructure.}
\par\plain {\f0\fs24\b0\i0 During 2011 we virtualized our three crawlers into six crawlers on top of VMware ESX servers. Fibre Channel connection to SAN thru one of webarchive servers had been replaced by NFSv3 connection to servers with direct access to GPFS. This unfortunately brought one new issue into our crawls. During broad crawl from multiple machines NFS server tended to hangup from too many connections and thus failing our crawl. Luckily we have three NFS servers, so we just spread two crawlers per one NFS server and problem ceased to exist. We are planing to experiment bit with NFSv4, but until we grow in number of our crawlers - we are not expecting any issues. Anther solution was buying licences for GPFS driver for our crawlers so they connect straight to SAN, but such approach was quite expensive as I heard from our IT department.}
\par\plain {\f0\fs24\b0\i0 But whole point of 2011 crawl was about splitting seeds into 5 crawls running in parallel fashion. This way we wanted to shorten crawl time from usual two month range to thinner date range, rendering our time-cut of Internet more precise. It actually worked pretty smooth, only less than half of our crawlers crashed during first week. But we had to stop crawling rest of the sites, because storage capacity for our regular crawls for next year was not certain.}
\par\plain {\f0\fs24\b0\i0 Parallel crawling on multiple machines have one strong issue which is ineffeciency in means of storage and visiting same URIs from different crawlers. I dont have any quantitative proof, but interlinked nature of web and specific of small country seemed like vague argument to care about.}
\par\plain \f0\fs24\b0\i0
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 Buying licence for GPFS driver on each crawler.}
\par\plain \f0\fs24\b0\i0
\par\plain \f0\fs24\b0\i0
\par\plain {\f0\fs24\b0\i0 Based on:}
\par\plain {\field{\*\fldinst HYPERLINK "http://tech.groups.yahoo.com/group/archive-crawler/message/7342"}{\fldrslt\f0\fs24\b0\i0 http://tech.groups.yahoo.com/group/archive-crawler/message/7342}}}