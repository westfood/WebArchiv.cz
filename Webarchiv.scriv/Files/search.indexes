<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="0">
            <Title>Draft</Title>
        </Document>
        <Document ID="1">
            <Title>Research</Title>
        </Document>
        <Document ID="2">
            <Title>Trash</Title>
        </Document>
        <Document ID="3">
            <Title>Heritrix 3 - usage</Title>
            <Text>Create two files for earch frequency from WAadmin, sort them with unique options into files seeds.txt in job direcotry.


heritrix@crawler00:/opt/heritrix/jobs/standard&gt; sort -u seeds-2012-09-* &gt; seeds.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; cp seeds.txt &gt; surts.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim surts.txt
:%s/^/+/g
:wq
</Text>
        </Document>
        <Document ID="4">
            <Title>Github</Title>
            <Text>In 2011 I get access to two physical machines which were harvesting web since 2008. Each one have been fitted with 20 TB RAID-6 storage and 8 GB RAM.  First crawler was dedicated for regular crawls, second one for whole domain or topic crawling. It actually worked quite well for our purposes, but biggest drawback of such approach was lengthy domain crawling.  Sometimes it took 3 or more months to crawl around 800k seeds and capture 5000 responses per domain. During such long crawl we encountered many unexpected incidents such as power blackout, Internet disconnection or corrupting crawl by running out of RAM . Regular checkpointing was therefore necessary to recover from failed crawl. But it was also necessary to pause crawl for checkpoint and it took unpleasant while before checkpoint has been written to disk and I could resume crawl. And in the end I also experienced that checkpoints failed to recover from time to time. Longer you crawl, easier you run into incident you need to deal with. There was even another argument for reducing time of our domain crawl. Libor Coufal who was head of Web Archive department in these times stressed that more condensed timeframe of harvest results in better validity of captured documents from curator’s perspective. Memento presentation I attended two years later proved him right to level that discrepancies between captured documents, could result in archived content which does not represent reality. 
Luckily maintenance of our servers has been running out and easiest solution seemed to run Heritrix as virtualized crawler. Library already invested in VMware ESX infrastructure, so whole bunch of old machines, Web Archive’s web and all curator’s application has been virtualized.  There was still quite a lot of capacity for other projects and our crawlers especially. So I prepared first three crawlers with 2 vCPUs and 8 GB RAM and run testing crawl. I didn’t do any benchmarking,  crawls just behaved well and performance felt right. So I ended up with six crawlers and possibility to dedicate five of them for domain crawling. 
For domain crawl I obtained Czech domain names from IXP nix.cz and split them as seeds into five parts and fed them to each crawler. Obvious problem was crawlers doesn’t have shared URI queue, so they will eventually harvest identical URIs founded during crawl, increasing overall harvest size as result. Anyway it was accepted behavior in sake of faster domain crawl. 
But new incident occurred during first week of crawl, crawlers ceased to write ARCs and end up in errors about no response from storage service. Before final collapse of crawl, errors were coming in waves. Heritrix deals quite nice with slow responses from storage, giving up writing for while, trying to write again later.  He care for storage in same way as he care for web servers, it is very gentle code. But in the end, crawlers died.  
After quick research I found cause in old pal NFS.  Old crawlers harvested ARCs to local storage and then I moved data to SAN over NFS, checking hashes after transfer. Once crawlers have been virtualized, I started to harvest straight to SAN over NFS. It seemed like jolly good approach to me, but I didn’t knew NFS have issues with too many data connections. After bit of experimenting I concluded that our NFS server configuration can handle only two crawlers without any errors. Quickest and cheapest options was to spread crawlers writing across three NFS servers. Which worked perfectly with our SAN configuration, because third NFS has been just installed and they are only access points into our storage.



Initializing Git repository:
heritrix@crawler00:/opt/heritrix/jobs&gt; 

git clone git://github.com/WebArchivCZ/Crawler-config.git

mkdir Crawler-config/Domain-crawls
cd Crawler-config/Domain-crawls

git checkout -b “Metadata”

vim crawler-beans.cxml
# Values changing with each crawl’
metadata.jobName=Serials 2013-01-1M
metadata.description=Pravidelná sklizeň semínek s každoměsíční frekvencí

# Stable values
metadata.operatorContactUrl=http://webarchiv.cz/kontakty/
metadata.operator=Rudolf Kreibich
metadata.operatorFrom=webarchiv@nkp.cz
metadata.organization=National Library of the Czech Republic - WebArchiv.cz
metadata.audience=WebArchiv.cz Users
metadata.userAgentTemplate=Mozilla/5.0 (compatible; heritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)
metadata.robotsPolicyName=ignore

git add crawler-beans.cxml
git -a -m &quot;Metadata configuration for crawling&quot;
git commit -a -m &quot;Metadata configuration for crawling&quot;
git checkout master
git merge Metadata
git branch -d Metadata

</Text>
        </Document>
        <Document ID="5">
            <Title>Regulerní výrazy a sheety</Title>
            <Text>&lt;!-- Noah's way --&gt;
&lt;!-- &lt;ref bean=&quot;hostConstraintRejectRegex-0&quot;/&gt; --&gt;
&lt;!-- and then top-level beans like so: --&gt;

&lt;bean id=&quot;rejectCalendars&quot; class=&quot;org.archive.modules.deciderules.MatchesListRegexDecideRule&quot; autowire-candidate=&quot;false&quot;&gt;
        &lt;property name=&quot;decision&quot; value=&quot;REJECT&quot;/&gt;
&lt;/bean&gt;

&lt;bean id=&quot;rejectCalendars-sheet&quot; class=&quot;org.archive.spring.Sheet&quot;&gt;
 &lt;property name=&quot;map&quot;&gt;
  &lt;map&gt;
   &lt;entry key=&quot;rejectCalendars.regexList&quot;&gt;
    &lt;list&gt;
        &lt;value&gt;.*agronavigator.cz*/kalendar.asp\?.*&lt;/value&gt;
        &lt;value&gt;.*agroporadenstvi.cz.*/kalendar.asp\?.*&lt;/value&gt;
        &lt;value&gt;http://zbraslav\.info/kalendar-akci/.*/201[5-9]/.*&lt;/value&gt;
        &lt;value&gt;.*&amp;amp;month:int=[0-9]{1,2}&amp;amp;year:int=(?!(2010|2011)).*&lt;/value&gt;
        &lt;value&gt;.*knihy_prehled\.asp\?rok=201[6-9]{1}.*&lt;/value&gt;
    &lt;/list&gt;
   &lt;/entry&gt;
  &lt;/map&gt;
 &lt;/property&gt;
&lt;/bean&gt;

&lt;bean class=&quot;org.archive.crawler.spring.SurtPrefixesSheetAssociation&quot;&gt;
 &lt;property name=&quot;surtPrefixes&quot;&gt;
  &lt;list&gt;
        &lt;value&gt;+fantasyplanet.cz&lt;/value&gt;
        &lt;value&gt;+agroporadenstvi.cz&lt;/value&gt;
        &lt;value&gt;+coena.cz&lt;/value&gt;
        &lt;value&gt;+agronavigator.cz&lt;/value&gt;
        &lt;value&gt;+zbraslav.info&lt;/value&gt;
  &lt;/list&gt;
 &lt;/property&gt;
 &lt;property name=&quot;targetSheetNames&quot;&gt;
  &lt;list&gt;
        &lt;value&gt;rejectCalendars-sheet&lt;/value&gt;
  &lt;/list&gt;
 &lt;/property&gt;
&lt;/bean&gt;
</Text>
        </Document>
        <Document ID="6">
            <Title>Příprava semínek</Title>
            <Text>heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim seeds-2012-10-V2M.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim seeds-2012-10-V1M.txt
sort -u seeds-2012-10-*.txt &gt;seeds.txt</Text>
        </Document>
        <Document ID="7">
            <Title>Transforming H1 to H3</Title>
        </Document>
        <Document ID="8">
            <Title>BUDGET: REPLENISH: FRONTIER: </Title>
            <Text>Viz. https://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets

frontier.queueTotalBudget=15000
# INSPECT: There is change that balanceReplenishAmount doesnt work as expected IE: Crawl 5k URIs, and wait for other seeds to crawl until 5k URIs - check FRONTIER
frontier.balanceReplenishAmount=5000


  &lt;property name=&quot;queueTotalBudget&quot; value=&quot;[see override above]&quot;
  &lt;property name=&quot;balanceReplenishAmount&quot; value=&quot;[see override above]&quot; /&gt;
…
&lt;!-- INSPECT! Does BaseQueuePrecedencePolicy works with balanceReplenishAmount? --!&gt;
 &lt;!-- &lt;property name=&quot;queuePrecedencePolicy&quot;&gt;
        &lt;bean class=&quot;org.archive.crawler.frontier.precedence.BaseQueuePrecedencePolicy&quot; /&gt;
       &lt;/property&gt; --&gt;

</Text>
        </Document>
        <Document ID="9">
            <Title>Introduction</Title>
            <Text>I get into web archiving two years ago, I had quite vague idea about topic and at first I thought I will do just crawls, help bit with of other things that curators deals with on technical level and focus in spare time on storage management issues as it looked way better approach to capitalize soon-to-gather knowledge. And I was right. If I spent more time on understanding storage concepts, my pay-roll would make me grin from ear to ear. But web archiving got me. During my first IIPC conference I realized web archiving deals with topics I was encountering during my New Media Studies. I had straight in front of me beautiful growing big data source with timestamps. Yey! Big companies deals with now and prognosis, with web archives we can dig history. But we have no users of big data. Nobody really cares about web archives. So we have tons of work to do.
But first comes first and first what comes is realization that consolidating web archive is big task alone. Apparently, lot of much more skilled people before me started lot of projects dealing with web archiving in library. Tools, databases, projects etc. And not much documentation to start with, at least in technical terms. Luckily last crawl engineering introduced me into Heritrix and other IT guy helped me with Wayback. Otherwise my start would be much more painful and slow. I can still ask them for help, but doing things is best way to learn things. So I head into switching to Heritrix 3, because its architecture sounded much more stable for future development. </Text>
        </Document>
        <Document ID="10">
            <Title>Document on github</Title>
            <Text>I started to commit changes of this document to Github. Tracking of changes makes sense for me and my supervisors:-) And it feels cool.</Text>
        </Document>
        <Document ID="11">
            <Title>Default behaviour</Title>
            <Text>Preference embed hops: Default behaviour is to get embedded content first. Which is right.

</Text>
        </Document>
        <Document ID="12">
            <Title>Distributed Crawling</Title>
            <Text>Our broad crawls within .cz domain counts around 1M initial seeds. Such crawls had been made in past years usually on one physical machine with attached RAID 6 storage. It was necessary to make checkpoints during crawl as Heritrix used to crash from various reasons starting from lack of RAM to failed components in our infrastructure.
During 2011 we virtualized our three crawlers into six crawlers on top of VMware ESX servers. Fibre Channel connection to SAN thru one of webarchive servers had been replaced by NFSv3 connection to servers with direct access to GPFS. This unfortunately brought one new issue into our crawls. During broad crawl from multiple machines NFS server tended to hangup from too many connections and thus failing our crawl. Luckily we have three NFS servers, so we just spread two crawlers per one NFS server and problem ceased to exist. We are planing to experiment bit with NFSv4, but until we grow in number of our crawlers - we are not expecting any issues. Anther solution was buying licences for GPFS driver for our crawlers so they connect straight to SAN, but such approach was quite expensive as I heard from our IT department.
But whole point of 2011 crawl was about splitting seeds into 5 crawls running in parallel fashion. This way we wanted to shorten crawl time from usual two month range to thinner date range, rendering our time-cut of Internet more precise. It actually worked pretty smooth, only less than half of our crawlers crashed during first week. But we had to stop crawling rest of the sites, because storage capacity for our regular crawls for next year was not certain.
Parallel crawling on multiple machines have one strong issue which is ineffeciency in means of storage and visiting same URIs from different crawlers. I dont have any quantitative proof, but interlinked nature of web and specific of small country seemed like vague argument to care about.


Buying licence for GPFS driver on each crawler.


Based on:
http://tech.groups.yahoo.com/group/archive-crawler/message/7342</Text>
        </Document>
    </Documents>
</SearchIndexes>
