<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="0">
            <Title>Draft</Title>
        </Document>
        <Document ID="1">
            <Title>Research</Title>
        </Document>
        <Document ID="2">
            <Title>Trash</Title>
        </Document>
        <Document ID="3">
            <Title>Heritrix 3 - usage</Title>
            <Text>Create two files for earch frequency from WAadmin, sort them with unique options into files seeds.txt in job direcotry.


heritrix@crawler00:/opt/heritrix/jobs/standard&gt; sort -u seeds-2012-09-* &gt; seeds.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; cp seeds.txt &gt; surts.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim surts.txt
:%s/^/+/g
:wq
</Text>
        </Document>
        <Document ID="4">
            <Title>Github</Title>
            <Text>When I get into webarchiving, we harvested web with two physical machines, each fitted with 20 TB RAID-6 storage,.  First crawler dedicated for regular crawls and second for whole domain or topic crawling. It actually worked quite well for our purposes, but biggest drawback of such approach was lengthy domain crawling.  It took sometimes 3+ month to crawl around 800k seeds and capture 5000 responses per domain. During such long time we could encountered many unexpected situation such as power blackout, DNS or Internet connection problems. Also there was small chance running Heritrix into out of RAM situation resulting in corruption of crawl. Without duplication reduction mechanism, checkpointing was necessary but bitterly time consuming in Heritrix 1. And on top of that checkpointed crawl failed to recover from time to time. Another argument for shortening our domain crawl came from curators perspective. Archived content should be in as condensed timeframe as possible, because we care about coherence of our time slice of web in particular.
Easiest solution seemed to run Heritrix as virtualized crawler. Library already invested in VMware ESX infrastructure, so whole bunch of old machines, webarchive’s web and all curator’s application has been virtualized before my arrival.  There was still quite a lot of capacity for of other projects and our crawlers. So I prepared first three crawlers with 2 vCPUs and 8 GB RAM and crawler performance seemed fine. There were no hard testing with comparation of numbers between virtual and physical machine, crawl just seemed to run as expected.  So I ended up with six crawlers and possibility to dedicate five of them for domain crawling.
So I split seeds into five parts and feed them to each crawler. Obvious problem was crawlers doesn’t have shared URI queue, so they will eventually harvest identical URIs founded during crawl and increasing overall harvest size. Anyway it was accepted behavior in sake of faster domain crawl. But new incident occurred during first week of crawl, our crawlers ceased to write ARCs and end up in errors about no response from storage service. Before final collapse of crawl, errors were coming in waves. Heritrix have quite nice way how to deal with slow responses from storage, giving up writing for while, trying to write again later.  It actually looks like same gentle approach it has towards web servers. But in the end, crawlers died anyway.  After quick research I found cause in old pal NFS. 
Before virtualization we harvested ARCs to local storage and then we moved data to SAN over NFS. Once crawlers have been virtualized, I started to harvest straight to SAN over NFS. It seemed like jolly good approach to me, but I didn’t knew NFS have issues with too many data connections. After bit of experimenting I concluded that our NFS server configuration can handle only two crawlers without any errors. Quickest and cheapest options was to spread crawlers writing across three NFS servers. Which worked perfectly with our SAN configuration, because third NFS has been just installed and they are only access points into our storage.



Initializing Git repository:
heritrix@crawler00:/opt/heritrix/jobs&gt; 

git clone git://github.com/WebArchivCZ/Crawler-config.git

mkdir Crawler-config/Domain-crawls
cd Crawler-config/Domain-crawls

git checkout -b “Metadata”

vim crawler-beans.cxml
# Values changing with each crawl’
metadata.jobName=Serials 2013-01-1M
metadata.description=Pravidelná sklizeň semínek s každoměsíční frekvencí

# Stable values
metadata.operatorContactUrl=http://webarchiv.cz/kontakty/
metadata.operator=Rudolf Kreibich
metadata.operatorFrom=webarchiv@nkp.cz
metadata.organization=National Library of the Czech Republic - WebArchiv.cz
metadata.audience=WebArchiv.cz Users
metadata.userAgentTemplate=Mozilla/5.0 (compatible; heritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)
metadata.robotsPolicyName=ignore

git add crawler-beans.cxml
git -a -m &quot;Metadata configuration for crawling&quot;
git commit -a -m &quot;Metadata configuration for crawling&quot;
git checkout master
git merge Metadata
git branch -d Metadata

</Text>
        </Document>
        <Document ID="5">
            <Title>Regulerní výrazy a sheety</Title>
            <Text>&lt;!-- Noah's way --&gt;
&lt;!-- &lt;ref bean=&quot;hostConstraintRejectRegex-0&quot;/&gt; --&gt;
&lt;!-- and then top-level beans like so: --&gt;

&lt;bean id=&quot;rejectCalendars&quot; class=&quot;org.archive.modules.deciderules.MatchesListRegexDecideRule&quot; autowire-candidate=&quot;false&quot;&gt;
        &lt;property name=&quot;decision&quot; value=&quot;REJECT&quot;/&gt;
&lt;/bean&gt;

&lt;bean id=&quot;rejectCalendars-sheet&quot; class=&quot;org.archive.spring.Sheet&quot;&gt;
 &lt;property name=&quot;map&quot;&gt;
  &lt;map&gt;
   &lt;entry key=&quot;rejectCalendars.regexList&quot;&gt;
    &lt;list&gt;
        &lt;value&gt;.*agronavigator.cz*/kalendar.asp\?.*&lt;/value&gt;
        &lt;value&gt;.*agroporadenstvi.cz.*/kalendar.asp\?.*&lt;/value&gt;
        &lt;value&gt;http://zbraslav\.info/kalendar-akci/.*/201[5-9]/.*&lt;/value&gt;
        &lt;value&gt;.*&amp;amp;month:int=[0-9]{1,2}&amp;amp;year:int=(?!(2010|2011)).*&lt;/value&gt;
        &lt;value&gt;.*knihy_prehled\.asp\?rok=201[6-9]{1}.*&lt;/value&gt;
    &lt;/list&gt;
   &lt;/entry&gt;
  &lt;/map&gt;
 &lt;/property&gt;
&lt;/bean&gt;

&lt;bean class=&quot;org.archive.crawler.spring.SurtPrefixesSheetAssociation&quot;&gt;
 &lt;property name=&quot;surtPrefixes&quot;&gt;
  &lt;list&gt;
        &lt;value&gt;+fantasyplanet.cz&lt;/value&gt;
        &lt;value&gt;+agroporadenstvi.cz&lt;/value&gt;
        &lt;value&gt;+coena.cz&lt;/value&gt;
        &lt;value&gt;+agronavigator.cz&lt;/value&gt;
        &lt;value&gt;+zbraslav.info&lt;/value&gt;
  &lt;/list&gt;
 &lt;/property&gt;
 &lt;property name=&quot;targetSheetNames&quot;&gt;
  &lt;list&gt;
        &lt;value&gt;rejectCalendars-sheet&lt;/value&gt;
  &lt;/list&gt;
 &lt;/property&gt;
&lt;/bean&gt;
</Text>
        </Document>
        <Document ID="6">
            <Title>Příprava semínek</Title>
            <Text>heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim seeds-2012-10-V2M.txt
heritrix@crawler00:/opt/heritrix/jobs/standard&gt; vim seeds-2012-10-V1M.txt
sort -u seeds-2012-10-*.txt &gt;seeds.txt</Text>
        </Document>
        <Document ID="7">
            <Title>Transforming H1 to H3</Title>
        </Document>
        <Document ID="8">
            <Title>BUDGET: REPLENISH: FRONTIER: </Title>
            <Text>Viz. https://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets

frontier.queueTotalBudget=15000
# INSPECT: There is change that balanceReplenishAmount doesnt work as expected IE: Crawl 5k URIs, and wait for other seeds to crawl until 5k URIs - check FRONTIER
frontier.balanceReplenishAmount=5000


  &lt;property name=&quot;queueTotalBudget&quot; value=&quot;[see override above]&quot;
  &lt;property name=&quot;balanceReplenishAmount&quot; value=&quot;[see override above]&quot; /&gt;
…
&lt;!-- INSPECT! Does BaseQueuePrecedencePolicy works with balanceReplenishAmount? --!&gt;
 &lt;!-- &lt;property name=&quot;queuePrecedencePolicy&quot;&gt;
        &lt;bean class=&quot;org.archive.crawler.frontier.precedence.BaseQueuePrecedencePolicy&quot; /&gt;
       &lt;/property&gt; --&gt;

</Text>
        </Document>
        <Document ID="9">
            <Title>Introduction</Title>
            <Text>I get into web archiving two years ago, I had quite vague idea about topic and at first I thought I will do just crawls, help bit with of other things that curators deals with on technical level and focus in spare time on storage management issues as it looked way better approach to capitalize soon-to-gather knowledge. And I was right. If I spent more time on understanding storage concepts, my pay-roll would make me grin from ear to ear. But web archiving got me. During my first IIPC conference I realized web archiving deals with topics I was encountering during my New Media Studies. I had straight in front of me beautiful growing big data source with timestamps. Yey! Big companies deals with now and prognosis, with web archives we can dig history. But we have no users of big data. Nobody really cares about web archives. So we have tons of work to do.
But first comes first and first what comes is realization that consolidating web archive is big task alone. Apparently, lot of much more skilled people before me started lot of projects dealing with web archiving in library. Tools, databases, projects etc. And not much documentation to start with, at least in technical terms. Luckily last crawl engineering introduced me into Heritrix and other IT guy helped me with Wayback. Otherwise my start would be much more painful and slow. I can still ask them for help, but doing things is best way to learn things. So I head into switching to Heritrix 3, because its architecture sounded much more stable for future development. </Text>
        </Document>
        <Document ID="10">
            <Title>Document on github</Title>
            <Text>I started to commit changes of this document to Github. Tracking of changes makes sense for me and my supervisors:-) And it feels cool.</Text>
        </Document>
        <Document ID="11">
            <Title>Default behaviour</Title>
            <Text>Preference embed hops: Default behaviour is to get embedded content first. Which is right.

</Text>
        </Document>
        <Document ID="12">
            <Title>Distributed Crawling</Title>
            <Text>Our broad crawls within .cz domain counts around 1M initial seeds. Such crawls had been made in past years usually on one physical machine with attached RAID 6 storage. It was necessary to make checkpoints during crawl as Heritrix used to crash from various reasons starting from lack of RAM to failed components in our infrastructure.
During 2011 we virtualized our three crawlers into six crawlers on top of VMware ESX servers. Fibre Channel connection to SAN thru one of webarchive servers had been replaced by NFSv3 connection to servers with direct access to GPFS. This unfortunately brought one new issue into our crawls. During broad crawl from multiple machines NFS server tended to hangup from too many connections and thus failing our crawl. Luckily we have three NFS servers, so we just spread two crawlers per one NFS server and problem ceased to exist. We are planing to experiment bit with NFSv4, but until we grow in number of our crawlers - we are not expecting any issues. Anther solution was buying licences for GPFS driver for our crawlers so they connect straight to SAN, but such approach was quite expensive as I heard from our IT department.
But whole point of 2011 crawl was about splitting seeds into 5 crawls running in parallel fashion. This way we wanted to shorten crawl time from usual two month range to thinner date range, rendering our time-cut of Internet more precise. It actually worked pretty smooth, only less than half of our crawlers crashed during first week. But we had to stop crawling rest of the sites, because storage capacity for our regular crawls for next year was not certain.
Parallel crawling on multiple machines have one strong issue which is ineffeciency in means of storage and visiting same URIs from different crawlers. I dont have any quantitative proof, but interlinked nature of web and specific of small country seemed like vague argument to care about.


Buying licence for GPFS driver on each crawler.


Based on:
http://tech.groups.yahoo.com/group/archive-crawler/message/7342</Text>
        </Document>
    </Documents>
</SearchIndexes>
